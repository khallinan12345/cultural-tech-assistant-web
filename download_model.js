import { exec } from 'child_process';import fs from 'fs';import path from 'path';import { fileURLToPath } from 'url';const __filename = fileURLToPath(import.meta.url);const __dirname = path.dirname(__filename);const modelsDir = path.join(__dirname, 'llama.cpp', 'models');// Make sure models directory existsif (!fs.existsSync(modelsDir)) {  fs.mkdirSync(modelsDir, { recursive: true });}console.log('Installing huggingface_hub Python package...');exec('pip install huggingface_hub', (err, stdout, stderr) => {  if (err) {    console.error('Error installing huggingface_hub:', stderr);    return;  }    console.log('Downloading Llama 2 model...');  const pythonScript = `from huggingface_hub import hf_hub_downloadimport osmodel_path = hf_hub_download(    repo_id='TheBloke/Llama-2-7B-Chat-GGUF',     filename='llama-2-7b-chat.Q4_K_M.gguf',    local_dir='${modelsDir.replace(/\\/g, '/')}')print(f"Model downloaded to: {model_path}")  `;    fs.writeFileSync('download_model.py', pythonScript);    exec('python download_model.py', (err, stdout, stderr) => {    if (err) {      console.error('Error downloading model:', stderr);      return;    }        console.log(stdout);    console.log('Model download complete!');        // Clean up the temporary Python script    fs.unlinkSync('download_model.py');  });});